{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import logging\n",
    "import csv\n",
    "import io\n",
    "import base64\n",
    "import ast\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas  \n",
    "from scipy.interpolate import griddata\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "\n",
    "\n",
    "from flask import Flask, jsonify, request, render_template\n",
    "from flask_cors import CORS \n",
    "from ragutils import ChatGpt, ragChat\n",
    "from langchain_openai import ChatOpenAI\n",
    "import importlib\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130343it [00:01, 122540.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['faucet', 'cap', 'plant', 'cabinet', 'bookshelf', 'knife', 'table', 'skateboard', 'mug', 'vase', 'pistol', 'flowerpot', 'clock', 'chair', 'bathtub', 'bottle', 'display', 'bag', 'trashbin', 'scissors', 'person', 'helmet', 'bowl', 'airplane', 'guitar', 'dresser', 'bed', 'sofa', 'bench', 'lamp'])\n",
      "faucet-7041\n",
      "cap-2094\n",
      "plant-2687\n",
      "cabinet-2578\n",
      "bookshelf-8247\n",
      "knife-4915\n",
      "table-46480\n",
      "skateboard-1980\n",
      "mug-1491\n",
      "vase-7016\n",
      "pistol-3660\n",
      "flowerpot-5625\n",
      "clock-6116\n",
      "chair-39479\n",
      "bathtub-8594\n",
      "bottle-4149\n",
      "display-8047\n",
      "bag-2726\n",
      "trashbin-4056\n",
      "scissors-715\n",
      "person-1768\n",
      "helmet-1451\n",
      "bowl-1852\n",
      "airplane-24645\n",
      "guitar-6823\n",
      "dresser-12808\n",
      "bed-6860\n",
      "sofa-29684\n",
      "bench-14045\n",
      "lamp-36758\n"
     ]
    }
   ],
   "source": [
    "modelDict = dict()\n",
    "with open ('./shapetalk.csv', 'r', encoding='gbk') as f:\n",
    "    counter = 0\n",
    "    print('initializing')\n",
    "    reader = csv.reader(f)\n",
    "    for row in tqdm(reader):\n",
    "        if (row[6] not in modelDict):\n",
    "            modelDict[row[6]] = set()\n",
    "        for i in range(1, 5):\n",
    "            if (row[i] != ''):\n",
    "                modelDict[row[6]].add(row[i])\n",
    "        counter += 1\n",
    "\n",
    "modelDict.pop('ShapeNet')\n",
    "modelDict.pop('source_object_class')\n",
    "print (modelDict.keys())\n",
    "\n",
    "vaseDocs = []\n",
    "vaseId = []\n",
    "i = 0\n",
    "# for sentence in list(modelDict['vase']):\n",
    "#     vaseDocs.append(Document(page_content=sentence))\n",
    "#     vaseId.append('vase' + str(i))\n",
    "#     i += 1\n",
    "source_knowledge_dict = {}\n",
    "for key in modelDict.keys():\n",
    "    print (key + '-' + str(len(list(modelDict[key]))))\n",
    "    source_knowledge_dict[key] = []\n",
    "    sentenCounter = 0\n",
    "    longSentences = ''\n",
    "    for sentence in modelDict[key]:\n",
    "        if (sentenCounter == 100):\n",
    "            source_knowledge_dict[key].append(longSentences)\n",
    "            longSentences = ''\n",
    "            sentenCounter = 0\n",
    "        longSentences += sentence + '\\n'\n",
    "        sentenCounter += 1\n",
    "    if (sentenCounter > 0):\n",
    "        source_knowledge_dict[key].append(longSentences)\n",
    "        longSentences = ''\n",
    "        sentenCounter = 0\n",
    "# print (source_knowledge_dict)\n",
    "# source_class = 'cap'\n",
    "# for sentence in modelDict['cap']:\n",
    "#     source_knowledge_cap += sentence + '\\n'\n",
    "# print(source_knowledge_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里会消耗巨量tokens，慎用\n",
    "# embed_model = OpenAIEmbeddings(base_url=\"https://api3.apifans.com/v1\")\n",
    "# vectorstore = Chroma.from_documents(documents=vaseDocs, embedding=embed_model, collection_name=\"model_discription_embed\")\n",
    "# print (vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHow can I change the vase?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m result \u001b[39m=\u001b[39m vectorstore\u001b[39m.\u001b[39msimilarity_search(query ,k \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m)\n\u001b[1;32m      3\u001b[0m result\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"How can I change the vase?\"\n",
    "result = vectorstore.similarity_search(query ,k = 5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "21\n",
      "27\n",
      "26\n",
      "83\n",
      "50\n",
      "465\n",
      "20\n",
      "15\n",
      "71\n",
      "37\n",
      "57\n",
      "62\n",
      "395\n",
      "86\n",
      "42\n",
      "81\n",
      "28\n",
      "41\n",
      "8\n",
      "18\n",
      "15\n",
      "19\n",
      "247\n",
      "69\n",
      "129\n",
      "69\n",
      "297\n",
      "141\n",
      "368\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for key in source_knowledge_dict.keys():\n",
    "    print (len(source_knowledge_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前进度：10/10"
     ]
    }
   ],
   "source": [
    "ragClient = ragChat()\n",
    "source_knowledge = 'The target\\'s foot is more round and the vase is with builtiful color, the top of it has a longger head\\nThe top of the vase is open\\nThe vase has flowers in it'\n",
    "modelClass = 'vase'\n",
    "prompt_template1 = f\"\"\"以下内容为对一个{modelClass}的描述集合，将以下内容中的每一句按照格式以键值字典的方式拆分，键为该物体的部位，如果是对整体的描述则直接用{modelClass}作为键，值为该部位对应的描述。请返回给我一个键值字典列表，列表中每一项为一个键值字典。不要使用代码格式返回:\n",
    "\n",
    "内容:\n",
    "{source_knowledge}\n",
    "\n",
    "\"\"\"\n",
    "ragClient.pushUserMessage(prompt_template1)\n",
    "\n",
    "# 以下操作消耗大量tokens\n",
    "# answer = ragClient.getGptMsg()\n",
    "# print (answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前进度：faucet-4"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "<html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.22.1</center>\r\n</body>\r\n</html>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 23\u001b[0m\n\u001b[1;32m     17\u001b[0m prompt_template1 \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m以下内容为对一个\u001b[39m\u001b[39m{\u001b[39;00mmodelClass\u001b[39m}\u001b[39;00m\u001b[39m的描述集合，将以下内容中的每一句按照格式以键值字典的方式拆分，键为该物体的部位，如果是对整体的描述则直接用\u001b[39m\u001b[39m{\u001b[39;00mmodelClass\u001b[39m}\u001b[39;00m\u001b[39m作为键，值为该部位对应的描述。请返回给我一个python键值字典列表，列表中每一项为一个键值字典。不要使用代码格式返回:\u001b[39m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[39m内容:\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[39m\u001b[39m\u001b[39m{\u001b[39;00msentences\u001b[39m}\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     22\u001b[0m ragClient\u001b[39m.\u001b[39mpushUserMessage(prompt_template1)\n\u001b[0;32m---> 23\u001b[0m answer \u001b[39m=\u001b[39m ragClient\u001b[39m.\u001b[39;49mgetGptMsg()\n\u001b[1;32m     24\u001b[0m parsedStr \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(answer)\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m dictEle \u001b[39min\u001b[39;00m parsedStr:\n",
      "File \u001b[0;32m/home/RAG_langchain-main/ragutils.py:65\u001b[0m, in \u001b[0;36mragChat.getGptMsg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetGptMsg\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     64\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''返回gpt的回复并自动加入当前对话列表（消耗tokens次数！别乱用）'''\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     receiveMsg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlangchinChat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmsgs)\u001b[39m.\u001b[39mcontent\n\u001b[1;32m     66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpushSystemMessage(receiveMsg)\n\u001b[1;32m     67\u001b[0m     \u001b[39mreturn\u001b[39;00m receiveMsg\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1017\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[39m@deprecated\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m0.1.7\u001b[39m\u001b[39m\"\u001b[39m, alternative\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minvoke\u001b[39m\u001b[39m\"\u001b[39m, removal\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m1.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m   1011\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1016\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[0;32m-> 1017\u001b[0m     generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m   1018\u001b[0m         [messages], stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   1019\u001b[0m     )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m   1020\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m   1021\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)  \u001b[39m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    634\u001b[0m                 m,\n\u001b[1;32m    635\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    636\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    637\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    852\u001b[0m             messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    853\u001b[0m         )\n\u001b[1;32m    854\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:705\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m     generation_info \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mdict\u001b[39m(raw_response\u001b[39m.\u001b[39mheaders)}\n\u001b[1;32m    704\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpayload)\n\u001b[1;32m    706\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/openai/resources/chat/completions.py:815\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    776\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    777\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    813\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    814\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    817\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    818\u001b[0m             {\n\u001b[1;32m    819\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    820\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    821\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m: audio,\n\u001b[1;32m    822\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    823\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    824\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    825\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    826\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    827\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_completion_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_completion_tokens,\n\u001b[1;32m    828\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    829\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    830\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodalities\u001b[39;49m\u001b[39m\"\u001b[39;49m: modalities,\n\u001b[1;32m    831\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    832\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    833\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    834\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    835\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    836\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mservice_tier\u001b[39;49m\u001b[39m\"\u001b[39;49m: service_tier,\n\u001b[1;32m    837\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    838\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstore\u001b[39;49m\u001b[39m\"\u001b[39;49m: store,\n\u001b[1;32m    839\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    840\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    841\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    842\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    843\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    844\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    845\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    846\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    847\u001b[0m             },\n\u001b[1;32m    848\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    849\u001b[0m         ),\n\u001b[1;32m    850\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    851\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    852\u001b[0m         ),\n\u001b[1;32m    853\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    854\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    855\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    856\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    955\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    956\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    957\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    958\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    959\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    960\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/openai/_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m remaining_retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1042\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1043\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1044\u001b[0m         input_options,\n\u001b[1;32m   1045\u001b[0m         cast_to,\n\u001b[1;32m   1046\u001b[0m         retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m   1047\u001b[0m         response_headers\u001b[39m=\u001b[39;49merr\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1048\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1049\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1050\u001b[0m     )\n\u001b[1;32m   1052\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/openai/_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1092\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1093\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1094\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1095\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1096\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1097\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1098\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/openai/_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m remaining_retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1042\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1043\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1044\u001b[0m         input_options,\n\u001b[1;32m   1045\u001b[0m         cast_to,\n\u001b[1;32m   1046\u001b[0m         retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m   1047\u001b[0m         response_headers\u001b[39m=\u001b[39;49merr\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1048\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1049\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1050\u001b[0m     )\n\u001b[1;32m   1052\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/openai/_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1092\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1093\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1094\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1095\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1096\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1097\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1098\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/openai/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.22.1</center>\r\n</body>\r\n</html>"
     ]
    }
   ],
   "source": [
    "# ragClient = ragChat()\n",
    "\n",
    "# dict_keys(['faucet', 'cap', 'plant', 'cabinet', 'bookshelf', 'knife', 'table', \n",
    "# 'skateboard', 'mug', 'vase', 'pistol', 'flowerpot', 'clock', 'chair', 'bathtub', \n",
    "# 'bottle', 'display', 'bag', 'trashbin', 'scissors', 'person', 'helmet', 'bowl', \n",
    "# 'airplane', 'guitar', 'dresser', 'bed', 'sofa', 'bench', 'lamp'])\n",
    "part2Discription = {}\n",
    "for modelClass in source_knowledge_dict.keys():\n",
    "    # 先不处理长度大于一百万的句子，tokens撑不住\n",
    "# modelClass = 'faucet'\n",
    "    # if (len(source_knowledge_dict[modelClass]) < 10):\n",
    "    discription = dict()\n",
    "    index = 0\n",
    "    for sentences in source_knowledge_dict[modelClass]:\n",
    "        if (index > 80):\n",
    "            break\n",
    "        print(f\"\\r当前进度：{modelClass}-{index}\", end='')\n",
    "        index += 1\n",
    "        prompt_template1 = f\"\"\"以下内容为对一个{modelClass}的描述集合，将以下内容中的每一句按照格式以键值字典的方式拆分，键为该物体的部位，如果是对整体的描述则直接用{modelClass}作为键，值为该部位对应的描述。请返回给我一个python键值字典列表，列表中每一项为一个键值字典。不要使用代码格式返回:\n",
    "    \n",
    "        内容:\n",
    "        {sentences}\n",
    "        \"\"\"\n",
    "        ragClient.pushUserMessage(prompt_template1)\n",
    "        answer = ragClient.getGptMsg()\n",
    "        parsedStr = json.loads(answer)\n",
    "        for dictEle in parsedStr:\n",
    "            for key in dictEle.keys():\n",
    "                if key in discription:\n",
    "                    discription[key].add(dictEle[key])\n",
    "                else:\n",
    "                    discription[key] = set()\n",
    "                    discription[key].add(dictEle[key])\n",
    "    for key in discription.keys():\n",
    "        discription[key] = list(discription[key])\n",
    "    part2Discription[modelClass] = discription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "print (part2Discription.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = '''[\n",
    "#   {\"foot\": \"more round\"},\n",
    "#   {\"vase\": \"with beautiful color\"},\n",
    "#   {\"top\": \"has a longer head\"},\n",
    "#   {\"top\": \"is open\"},\n",
    "#   {\"vase\": \"has flowers in it\"},\n",
    "#   {\"top\": \"has a longer head\"}]'''\n",
    "# answer += ''\n",
    "# print (answer)\n",
    "# print (answer)\n",
    "parsedStr = json.loads(answer)\n",
    "\n",
    "vasePart2Discription = {}\n",
    "for dictEle in parsedStr:\n",
    "    for key in dictEle.keys():\n",
    "        if key in vasePart2Discription:\n",
    "            vasePart2Discription[key].add(dictEle[key])\n",
    "        else:\n",
    "            vasePart2Discription[key] = set()\n",
    "            vasePart2Discription[key].add(dictEle[key])\n",
    "# print (vasePart2Discription)\n",
    "for key in vasePart2Discription.keys():\n",
    "    vasePart2Discription[key] = list(vasePart2Discription[key])\n",
    "with open(source_class + '_Part2Discription', 'w') as f:\n",
    "    json.dump(vasePart2Discription, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据所提供的描述，你可以对该物品的以下部位做改动：\n",
      "\n",
      "1. **帽檐（brim）**：\n",
      "   - 改变形状，例如将帽檐从圆形改为方形，或调整帽檐的宽度和厚度。\n",
      "   - 调整帽檐的弯曲度，例如使其更平坦或更弯曲。\n",
      "   - 增加或减少帽檐的长度或高度。\n",
      "\n",
      "2. **帽舌（visor）**：\n",
      "   - 改变帽舌的形状，例如从圆形改为方形或其他几何形状。\n",
      "   - 调整帽舌的厚度或长度。\n",
      "   - 改变帽舌的弯曲度或位置。\n",
      "\n",
      "3. **顶部（top）**：\n",
      "   - 改变顶部的形状，例如从尖顶改为平顶或圆顶。\n",
      "   - 调整顶部的直径或高度。\n",
      "   - 添加或移除顶部的饰物或标志。\n",
      "\n",
      "4. **边缘（rim）**：\n",
      "   - 调整边缘的高度或厚度。\n",
      "   - 改变边缘的形状或弯曲度。\n",
      "\n",
      "5. **按扣（button）**：\n",
      "   - 改变按扣的大小或形状。\n",
      "   - 添加或移除按扣。\n",
      "\n",
      "这些改动可以根据个人喜好和风格需求进行调整，从而改变帽子的外观和功能。\n"
     ]
    }
   ],
   "source": [
    "# 1：这个是基于用户问答做反馈\n",
    "query = '请问我可以对这个物品的那些部位做改动？'\n",
    "prompt_template2 = f\"\"\"请基于以下内容回答问题\n",
    "\n",
    "内容:\n",
    "{vasePart2Discription}\n",
    "query:\n",
    "{query}\n",
    "\"\"\"\n",
    "ragClient.pushUserMessage(prompt_template2)\n",
    "answer = ragClient.getGptMsg()\n",
    "print (answer)\n",
    "\n",
    "# 2：对用户输入prompt进行编码，做embedding相似的编辑方向推荐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('myconda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc93251d6f721491c6fbc0c6bcbe4e9a605d28f77b9a8e19b15e1c61d18557e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
